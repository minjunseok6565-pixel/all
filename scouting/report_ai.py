from __future__ import annotations

"""LLM writer for scouting reports.

This module is intentionally isolated so the DB/service logic can be tested
without requiring a network call. In production, report generation happens at
month-end checkpoint (Option A).

Implementation uses Google Generative AI (Gemini), matching existing project
modules (news_ai.py, season_report_ai.py).
"""

import datetime as _dt
import json
from typing import Any, Dict, Tuple

try:
    import google.generativeai as genai
except Exception:  # pragma: no cover - optional dependency in offline/local env
    genai = None


DEFAULT_MODEL_NAME = "gemini-3-pro-preview"
PROMPT_VERSION = "scouting_report_v1"


def _now_iso() -> str:
    return _dt.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


def _extract_text_from_gemini_response(resp: Any) -> str:
    """Robustly extract plain text from gemini response."""
    # Common shape: response.text
    try:
        t = getattr(resp, "text", None)
        if isinstance(t, str) and t.strip():
            return t
    except Exception:
        pass

    # Fallback: iterate candidates/parts
    try:
        cands = getattr(resp, "candidates", None)
        if isinstance(cands, list) and cands:
            parts = getattr(cands[0].content, "parts", None)
            if isinstance(parts, list) and parts:
                texts = []
                for p in parts:
                    tx = getattr(p, "text", None)
                    if isinstance(tx, str) and tx:
                        texts.append(tx)
                if texts:
                    return "\n".join(texts)
    except Exception:
        pass

    # Ultimate fallback
    return str(resp) if resp is not None else ""


def _build_prompt(payload: Dict[str, Any]) -> str:
    """Return a Korean scouting report prompt with the structured payload."""
    # Keep JSON fairly compact to reduce tokens.
    payload_json = json.dumps(payload, ensure_ascii=False, separators=(",", ":"), sort_keys=True)

    return (
        "ë‹¹ì‹ ì€ NBA êµ¬ë‹¨ ìŠ¤ì¹´ìš°í„°ë‹¤. ì•„ëž˜ì˜ êµ¬ì¡°í™”ëœ ë°ì´í„°(payload)ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ìŠ¤ì¹´ìš°íŒ… ë¦¬í¬íŠ¸'ë¥¼ ìž‘ì„±í•˜ë¼.\n"
        "- ì¶œë ¥ ì–¸ì–´: í•œêµ­ì–´\n"
        "- í˜•ì‹: Markdown í…ìŠ¤íŠ¸(ì œëª©/ì†Œì œëª©/ë¶ˆë¦¿). í‘œ(í…Œì´ë¸”)ëŠ” ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ.\n"
        "- ê¸¸ì´: 700~1400ìž ë‚´ì™¸(í•µì‹¬ë§Œ, ê³¼ë„í•˜ê²Œ ê¸¸ê²Œ ì“°ì§€ ë§ ê²ƒ).\n"
        "- ë°ì´í„°ì— ì—†ëŠ” ì‚¬ì‹¤ì„ ë‹¨ì •/ì¶”ê°€í•˜ì§€ ë§ ê²ƒ(íŠ¹ížˆ í”Œë ˆì´ ìŠ¤íƒ€ì¼/ìƒëŒ€ ë ˆë²¨/ë¶€ìƒ/ì„±ê²©).\n"
        "- ìˆ«ìž 0~100 ì ìˆ˜(mu/sigma)ë¥¼ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ì§€ ë§ ê²ƒ. ëŒ€ì‹  tier(íŠ¹ê¸‰/ê°•ì /í‰ê· ê¶Œ/ìš°ë ¤/ê²½ê³ ) + confidence(high/medium/low) + range_textë¥¼ ì‚¬ìš©.\n"
        "- evidence_tagsëŠ” ë°˜ë“œì‹œ ê·¼ê±°ë¡œ í™œìš©í•˜ë¼. PLUS/MINUS/QUESTION/METAì˜ kindë¥¼ ì¡´ì¤‘í•˜ê³ , tagsì— ì—†ëŠ” ê·¼ê±°ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ ê²ƒ.\n"
        "- college_context.stat_lineê³¼ notesëŠ” 'ë¬¸ìž¥ì— ë„£ì–´ë„ ë˜ëŠ” ìœ ì¼í•œ ë°•ìŠ¤ìŠ¤ì½”ì–´ ê·¼ê±°'ë¡œ ê°„ì£¼í•œë‹¤.\n"
        "- ì´ ë¦¬í¬íŠ¸ëŠ” scout.focus_signals ì¤‘ì‹¬ì˜ ë¶€ë¶„ ê´€ì°°ì¼ ìˆ˜ ìžˆë‹¤. ê·¸ ì ì„ ìš”ì•½ì— í•œ ë²ˆ ì–¸ê¸‰í•˜ë¼.\n"
        "\n"
        "ë°˜ë“œì‹œ í¬í•¨í•  ì„¹ì…˜(ìˆœì„œ ìœ ì§€):\n"
        "1) ìš”ì•½ (2~3ë¬¸ìž¥)\n"
        "2) ì´ë²ˆ ë‹¬ì— í™•ì‹¤í•´ì§„ ê²ƒ (delta_since_last.new_info ê¸°ë°˜, ì—†ìœ¼ë©´ 'í° ë³€í™” ì—†ìŒ/í‘œë³¸ ë¶€ì¡±'ë¡œ)\n"
        "3) í”„ë¡œìŠ¤íŽ™íŠ¸ ì‹ í˜¸ ì¹´ë“œ (signalsë¥¼ group(offense/defense/physical)ë³„ë¡œ ë¬¶ê³ , ê° signalë§ˆë‹¤ ì•„ëž˜ í¬ë§·)\n"
        "   - **{label}** â€” {tier} Â· ì‹ ë¢°ë„ {confidence} Â· {range_text}\n"
        "     - âœ… (PLUS ê·¼ê±° 1~2ê°œ)\n"
        "     - âš ï¸ (MINUS ê·¼ê±° 0~1ê°œ)\n"
        "     - â“ (QUESTION 0~1ê°œ)\n"
        "     - ðŸ§¾ (META 0~1ê°œ; í‘œë³¸/íˆ´-ìƒì‚° ê´´ë¦¬ ë“±)\n"
        "4) NBA ë²ˆì—­ (1~2ë¬¸ë‹¨: ì–´ë–¤ ì—­í• ë¡œ ì“°ì¼ì§€/ë¬´ì—‡ì´ íŠ¸ë¦¬ê±°ì¸ì§€/ë¬´ì—‡ì´ ìƒí•œì„ ë§‰ëŠ”ì§€)\n"
        "5) ë¦¬ìŠ¤í¬/ì§ˆë¬¸ (watchlist_questionsë¥¼ ë¶ˆë¦¿ 2~3ê°œë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n"
        "6) ë‹¤ìŒ ì²´í¬ í¬ì¸íŠ¸ (2~3ê°œ: ë‹¤ìŒ ë‹¬ì— ë¬´ì—‡ì„ ë³´ë©´ ì‹ ë¢°ë„ê°€ ì˜¤ë¥´ëŠ”ì§€)\n"
        "\n"
        "[payload JSON]\n"
        f"{payload_json}\n"
    )


class ScoutingReportWriter:
    """Thin wrapper around Gemini model to generate report_text."""

    def __init__(self, *, api_key: str, model_name: str = DEFAULT_MODEL_NAME):
        if genai is None:
            raise ImportError(
                "google.generativeai is required to generate scouting reports. "
                "Install the optional dependency to enable this feature."
            )
        if not api_key or not str(api_key).strip():
            raise ValueError("api_key is required")
        self.api_key = str(api_key).strip()
        self.model_name = str(model_name).strip() or DEFAULT_MODEL_NAME

        # Configure globally for this process. This matches other modules.
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(self.model_name)

    def write(self, payload: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
        prompt = _build_prompt(payload)

        # Note: We intentionally keep generation config default for now.
        # If you need more deterministic output, set temperature=0.2 etc.
        resp = self.model.generate_content(prompt)
        text = _extract_text_from_gemini_response(resp).strip()

        meta: Dict[str, Any] = {
            "model": self.model_name,
            "prompt_version": PROMPT_VERSION,
            "generated_at": _now_iso(),
        }

        # Try to capture usage metadata if available
        try:
            usage = getattr(resp, "usage_metadata", None)
            if usage is not None:
                # Convert to JSON-serializable dict-ish
                meta["usage_metadata"] = json.loads(json.dumps(usage, default=str))
        except Exception:
            pass

        return text, meta
